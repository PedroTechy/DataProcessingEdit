{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTechy/DataProcessingEdit/blob/main/spark_streaming/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/content\n",
        "!rm -rf /content/lake/silver"
      ],
      "metadata": {
        "id": "yYUn0eP80k8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "7e3ce799-7098-4c84-f568-738f0e6ab466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "e2fe7f79-3b52-49b4-dbf0-0e67403fd289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "aeeb17d5-6662-487d-cb4b-fee120f34dff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".option('checkpointLocation', '/content/lake/bronze/messages/checkpoint')\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWQExsnzlMFe"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"/content/lake/bronze/messages/data\")\n",
        "#df.show() #usefull to visualize the data and understand the schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZAHIZeZMlpoH"
      },
      "outputs": [],
      "source": [
        "#Step 2\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Can not use readStreamwithout  data schema, defining that first\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"value\", LongType(), True),\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"message_id\", StringType(), True),\n",
        "    StructField(\"channel\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Getting the types of the schema rights required looking at some errors printed when writing to silver\n",
        "\n",
        "bronze_df = spark.readStream.format(\"parquet\").schema(schema).load(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "# Join with countries dataset\n",
        "joined_df = bronze_df.join(countries, on=\"country_id\", how=\"left\")\n",
        "\n",
        "# Split into messages_corrupted and messages (based on the exploration done before, for correupted values i have \"\", \"NONE\" and in case lets also clean the nulls (None))\n",
        "messages_corrupted_df = joined_df.filter(col(\"event_type\").isin([None, \"\", \"NONE\"]))\n",
        "messages_df = joined_df.filter(col(\"event_type\").isin([\"CLICKED\", \"CREATED\", \"RECEIVED\", \"OPEN\", \"SENT\"]))\n",
        "\n",
        "# NOTE: I actualy tried to avoid this solution for messages_df as its not elegant at all.\n",
        "# I had done it using subtract (joined_df.subtract(messages_corrupted_df) or joined_df.join(messages_corrupted_df, on=[\"message_id\", \"event_type\"], how=\"left_anti\")~\n",
        "# but both cases are not possible. I also tried negating messages_corrupted_df, using joined_df.filter(~col(\"event_type\").isin([None, \"\", \"NONE\"]))\n",
        "# but this resulted in 0 rows (not sure why :/)\n",
        "\n",
        "\"\"\"print(messages_corrupted_df.count())\n",
        "print(messages_df.count())\n",
        "print(joined_df.count())\n",
        "print(messages_df.show())\n",
        "print(messages_corrupted_df.show())\"\"\"\n",
        "\n",
        "# Add date column using the timestamp already provided\n",
        "messages_corrupted_df = messages_corrupted_df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
        "messages_df = messages_df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
        "\n",
        "# Given that we are writing under the same conditions, but just for two different locations, a functions saves us time\n",
        "def write_to_silver(df, data_path, checkpoint_location):\n",
        "    query = (df.writeStream\n",
        "             .format(\"parquet\")\n",
        "             .outputMode(\"append\")\n",
        "             .partitionBy(\"date\")\n",
        "             .option(\"checkpointLocation\", checkpoint_location)\n",
        "             .trigger(processingTime=\"5 seconds\")\n",
        "             .start(data_path))\n",
        "    return query\n",
        "\n",
        "query_corrupted = write_to_silver(messages_corrupted_df, \"/content/lake/silver/messages_corrupted/data\", \"/content/lake/silver/messages_corrupted/checkpoint\")\n",
        "query_messages = write_to_silver(messages_df, \"/content/lake/silver/messages/data\", \"/content/lake/silver/messages/checkpoint\")\n",
        "\n",
        "# Run streaming for at least 20 seconds\n",
        "query_corrupted.awaitTermination(20)\n",
        "query_messages.awaitTermination(20)\n",
        "\n",
        "# Stop streaming queries\n",
        "query_corrupted.stop()\n",
        "query_messages.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "outputId": "0bf554bf-f6bb-442d-d2cb-95903e819259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data validation successful: Bronze count matches Silver count\n"
          ]
        }
      ],
      "source": [
        "# To check the results i will simply read them as batch and count. I could have read as strem and \"freeze\" but this is simpler I guess\n",
        "# Count bronze records\n",
        "bronze_count = spark.read.format(\"parquet\").load(\"/content/lake/bronze/messages/data\").count()\n",
        "\n",
        "# Count silver records\n",
        "silver_messages_count = spark.read.schema(schema).format(\"parquet\").load(\"/content/lake/silver/messages/data\").count()\n",
        "silver_corrupted_count = spark.read.schema(schema).format(\"parquet\").load(\"/content/lake/silver/messages_corrupted/data\").count()\n",
        "\n",
        "# Compare counts\n",
        "if bronze_count == silver_messages_count + silver_corrupted_count:\n",
        "    print(\"Data validation successful: Bronze count matches Silver count\")\n",
        "else:\n",
        "    print(\"Data validation failed: Bronze count does not match Silver count\")\n",
        "    print(f\"Bronze count: {bronze_count}\")\n",
        "    print(f\"Silver messages count: {silver_messages_count}\")\n",
        "    print(f\"Silver corrupted count: {silver_corrupted_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "R3J9XyOHhqvU",
        "outputId": "8e574355-de4a-4d42-f711-87dca0bae6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|      country|      date|row_number|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+----------+\n",
            "|      2007|2024-12-16 20:38:...|    4|      SENT|0904970d-3189-40d...|  OTHER|   1019|United States|2024-12-16|         1|\n",
            "|      2007|2024-12-16 20:38:...|    5|      SENT|0904970d-3189-40d...|  OTHER|   1019|United States|2024-12-16|         2|\n",
            "|      2012|2024-12-16 20:38:...|    7|      SENT|0904970d-3189-40d...|    SMS|   1002|        India|2024-12-16|         1|\n",
            "|      2009|2024-12-16 20:38:...|   34|   CLICKED|17b6084e-af9e-438...|   CHAT|   1046|    Australia|2024-12-16|         1|\n",
            "|      2013|2024-12-16 20:39:...|   80|      SENT|17b6084e-af9e-438...|  OTHER|   1019|  South Korea|2024-12-16|         1|\n",
            "|      2004|2024-12-16 20:39:...|   91|   CLICKED|182dbfe7-a8cc-4c8...|   PUSH|   1031|       France|2024-12-16|         1|\n",
            "|      2003|2024-12-16 20:39:...|   47|   CREATED|182dbfe7-a8cc-4c8...|   CHAT|   1010|      Germany|2024-12-16|         1|\n",
            "|      2012|2024-12-16 20:40:...|  119|      SENT|182dbfe7-a8cc-4c8...|   CHAT|   1046|        India|2024-12-16|         1|\n",
            "|      2005|2024-12-16 20:38:...|   17|      SENT|182dbfe7-a8cc-4c8...|  EMAIL|   1033|        Italy|2024-12-16|         1|\n",
            "|      2005|2024-12-16 20:38:...|   18|   CREATED|1940abf2-0096-407...|   CHAT|   1037|        Italy|2024-12-16|         1|\n",
            "|      2003|2024-12-16 20:39:...|   99|      OPEN|1940abf2-0096-407...|   CHAT|   1016|      Germany|2024-12-16|         1|\n",
            "|      2009|2024-12-16 20:38:...|   25|  RECEIVED|1940abf2-0096-407...|   PUSH|   1023|    Australia|2024-12-16|         1|\n",
            "|      2008|2024-12-16 20:39:...|   74|  RECEIVED|1e7b9642-0aa9-496...|   PUSH|   1030|       Canada|2024-12-16|         1|\n",
            "|      2012|2024-12-16 20:38:...|   13|   CREATED|235ce738-98e2-4b7...|  EMAIL|   1021|        India|2024-12-16|         1|\n",
            "|      2001|2024-12-16 20:39:...|   43|   CREATED|235ce738-98e2-4b7...|  OTHER|   1017|     Portugal|2024-12-16|         1|\n",
            "|      2007|2024-12-16 20:38:...|   40|  RECEIVED|235ce738-98e2-4b7...|   CHAT|   1009|United States|2024-12-16|         1|\n",
            "|      2004|2024-12-16 20:39:...|   87|   CREATED|2632c6c2-6e38-43e...|  OTHER|   1026|       France|2024-12-16|         1|\n",
            "|      2013|2024-12-16 20:40:...|  107|      SENT|2632c6c2-6e38-43e...|  OTHER|   1014|  South Korea|2024-12-16|         1|\n",
            "|      2001|2024-12-16 20:40:...|  103|  RECEIVED|2794386e-038a-4a1...|    SMS|   1016|     Portugal|2024-12-16|         1|\n",
            "|      2015|2024-12-16 20:38:...|   10|      OPEN|27b7cfa5-8a26-467...|   PUSH|   1014|    Argentina|2024-12-16|         1|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|      country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "|      2007|2024-12-16 20:38:...|    4|      SENT|0904970d-3189-40d...|  OTHER|   1019|United States|2024-12-16|\n",
            "|      2012|2024-12-16 20:38:...|    7|      SENT|0904970d-3189-40d...|    SMS|   1002|        India|2024-12-16|\n",
            "|      2009|2024-12-16 20:38:...|   34|   CLICKED|17b6084e-af9e-438...|   CHAT|   1046|    Australia|2024-12-16|\n",
            "|      2013|2024-12-16 20:39:...|   80|      SENT|17b6084e-af9e-438...|  OTHER|   1019|  South Korea|2024-12-16|\n",
            "|      2004|2024-12-16 20:39:...|   91|   CLICKED|182dbfe7-a8cc-4c8...|   PUSH|   1031|       France|2024-12-16|\n",
            "|      2003|2024-12-16 20:39:...|   47|   CREATED|182dbfe7-a8cc-4c8...|   CHAT|   1010|      Germany|2024-12-16|\n",
            "|      2012|2024-12-16 20:40:...|  119|      SENT|182dbfe7-a8cc-4c8...|   CHAT|   1046|        India|2024-12-16|\n",
            "|      2005|2024-12-16 20:38:...|   17|      SENT|182dbfe7-a8cc-4c8...|  EMAIL|   1033|        Italy|2024-12-16|\n",
            "|      2005|2024-12-16 20:38:...|   18|   CREATED|1940abf2-0096-407...|   CHAT|   1037|        Italy|2024-12-16|\n",
            "|      2003|2024-12-16 20:39:...|   99|      OPEN|1940abf2-0096-407...|   CHAT|   1016|      Germany|2024-12-16|\n",
            "|      2009|2024-12-16 20:38:...|   25|  RECEIVED|1940abf2-0096-407...|   PUSH|   1023|    Australia|2024-12-16|\n",
            "|      2008|2024-12-16 20:39:...|   74|  RECEIVED|1e7b9642-0aa9-496...|   PUSH|   1030|       Canada|2024-12-16|\n",
            "|      2012|2024-12-16 20:38:...|   13|   CREATED|235ce738-98e2-4b7...|  EMAIL|   1021|        India|2024-12-16|\n",
            "|      2001|2024-12-16 20:39:...|   43|   CREATED|235ce738-98e2-4b7...|  OTHER|   1017|     Portugal|2024-12-16|\n",
            "|      2007|2024-12-16 20:38:...|   40|  RECEIVED|235ce738-98e2-4b7...|   CHAT|   1009|United States|2024-12-16|\n",
            "|      2004|2024-12-16 20:39:...|   87|   CREATED|2632c6c2-6e38-43e...|  OTHER|   1026|       France|2024-12-16|\n",
            "|      2013|2024-12-16 20:40:...|  107|      SENT|2632c6c2-6e38-43e...|  OTHER|   1014|  South Korea|2024-12-16|\n",
            "|      2001|2024-12-16 20:40:...|  103|  RECEIVED|2794386e-038a-4a1...|    SMS|   1016|     Portugal|2024-12-16|\n",
            "|      2015|2024-12-16 20:38:...|   10|      OPEN|27b7cfa5-8a26-467...|   PUSH|   1014|    Argentina|2024-12-16|\n",
            "|      2014|2024-12-16 20:39:...|   90|      SENT|27b7cfa5-8a26-467...|   PUSH|   1042|       Russia|2024-12-16|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dedup data\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "# using row_number to then filter by the 1st one regarding the time stamp\n",
        "partitioned_df = df.withColumn(\"row_number\", row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\")\n",
        "                                                              .orderBy(\"timestamp\")))\n",
        "\n",
        "partitioned_df.show()   # the dups can be seen here\n",
        "\n",
        "# Now that we have a row number that \"ranks\" by tiem stamp, we kep only those with row_number as 1, then we delete row_number column because we dont need it anymore\n",
        "dedup = partitioned_df.filter(\"row_number = 1\").drop(\"row_number\")\n",
        "\n",
        "dedup.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "outputId": "76137011-fe23-441b-f7df-02f51931d5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-16|   PUSH|      4|      2|   1|       5|   2|\n",
            "|2024-12-16|   CHAT|      2|      6|   3|       5|   6|\n",
            "|2024-12-16|  OTHER|      4|      4|   1|       3|   5|\n",
            "|2024-12-16|  EMAIL|      3|      3|   3|    NULL|   5|\n",
            "|2024-12-16|    SMS|      2|      2|   2|       9|   4|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 1\n",
        "from pyspark.sql.functions import col, count, to_date, date_format\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# First we group the data by date and channel. The date_format function is used to extract the date from the timestamp column.\n",
        "# Then, we use the pivot function to transform the event_type column into separate columns for each event type, with the values representing the count of messages for that event type.\n",
        "# We aggregate the data by counting the number of messages for each combination of date, channel, and event_type.\n",
        "\n",
        "report1 = dedup.groupBy(date_format(col(\"timestamp\"), \"yyyy-MM-dd\").alias(\"date\"), \"channel\") \\\n",
        "              .pivot(\"event_type\", ['CLICKED', 'CREATED', 'OPEN', 'RECEIVED', 'SENT']) \\\n",
        "              .agg(count(\"*\"))\n",
        "\n",
        "report1.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "outputId": "38510d2b-b662-48ef-e87c-10545f54a78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|   1033|         5|   1|    1|    2|   0|  1|\n",
            "|   1028|         4|   0|    2|    2|   0|  0|\n",
            "|   1010|         4|   1|    1|    0|   0|  2|\n",
            "|   1023|         4|   0|    2|    0|   2|  0|\n",
            "|   1025|         3|   2|    0|    0|   0|  1|\n",
            "|   1016|         3|   1|    0|    0|   0|  2|\n",
            "|   1034|         3|   0|    0|    1|   1|  1|\n",
            "|   1019|         3|   1|    0|    2|   0|  0|\n",
            "|   1046|         3|   3|    0|    0|   0|  0|\n",
            "|   1008|         3|   0|    0|    0|   1|  2|\n",
            "|   1002|         3|   1|    0|    1|   0|  1|\n",
            "|   1037|         3|   3|    0|    0|   0|  0|\n",
            "|   1039|         3|   1|    0|    0|   2|  0|\n",
            "|   1038|         3|   1|    1|    0|   0|  1|\n",
            "|   1014|         3|   0|    0|    1|   1|  1|\n",
            "|   1005|         2|   0|    1|    0|   1|  0|\n",
            "|   1031|         2|   0|    1|    0|   1|  0|\n",
            "|   1032|         2|   0|    0|    1|   0|  1|\n",
            "|   1045|         2|   1|    1|    0|   0|  0|\n",
            "|   1015|         2|   0|    0|    0|   1|  1|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 2\n",
        "\n",
        "from pyspark.sql.functions import col, count, sum\n",
        "\n",
        "# First we group the data by user_id to analyze activity by each user\n",
        "# Then we calculate the total number of interactions for each user (called \"iterations\") by coutining how many times each channel is used\n",
        "# and tehn we sum all counts for that channel and for that user\n",
        "# Finally, we sort the results by the total number of interactions in descending order so we can easily see which users are the most active.\n",
        "\n",
        "report2 = dedup \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .agg(count(\"*\").alias(\"iterations\"),\n",
        "         sum(F.when(col(\"channel\") == \"CHAT\", 1).otherwise(0)).alias(\"CHAT\"),\n",
        "         sum(F.when(col(\"channel\") == \"EMAIL\", 1).otherwise(0)).alias(\"EMAIL\"),\n",
        "         sum(F.when(col(\"channel\") == \"OTHER\", 1).otherwise(0)).alias(\"OTHER\"),\n",
        "         sum(F.when(col(\"channel\") == \"PUSH\", 1).otherwise(0)).alias(\"PUSH\"),\n",
        "         sum(F.when(col(\"channel\") == \"SMS\", 1).otherwise(0)).alias(\"SMS\")) \\\n",
        "    .orderBy(col(\"iterations\").desc())\n",
        "\n",
        "report2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n",
        "\"\"\"\n",
        "R: For this use case, Spark Structured Streaming is a strong contender due to its ability to handle low-latency processing, a key requirement here.\n",
        "However, Spark's strengths lie in its processing capabilities, not necessarily in direct data ingestion or output.\n",
        "Therefore, a more robust solution would involve using Spark alongside a message broker like Kafka, Pub/Sub, or Kinesis for efficient and low-latency data ingestion.\n",
        "\n",
        "Once Spark processes the data, the aggregated results need to be stored in a system optimized for frequent,\n",
        "low-latency queries by the dashboard. Options include databases like Cassandra or MongoDB, or data lakes designed for fast reads like Delta Lake should be a good option.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}